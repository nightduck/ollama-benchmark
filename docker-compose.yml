version: '3.10'

services:
  llm_benchmark:
    image: llm_benchmark:jetson
    container_name: llm_benchmark
    # Required for the container to access the Jetson GPU
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      # Mount a local directory to persist models
      - ollama:/root/.ollama
      # Mount the host's nv_tegra_release file to help Ollama detect the Jetpack version
      - /etc/nv_tegra_release:/etc/nv_tegra_release:ro
    ports:
      # Map port 11434 on the host to the container's port
      - "11434:11434"
    restart: no
    # If running on a Jetpack 5 or 6 system, explicitly set the version
    environment:
      - JETSON_JETPACK=5 # or 6

volumes:
  ollama:
    external: true